{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4fe8b2",
   "metadata": {
    "papermill": {
     "duration": 0.007395,
     "end_time": "2024-05-13T07:20:43.335004",
     "exception": false,
     "start_time": "2024-05-13T07:20:43.327609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Firstly we install the pyspellcheck library. This library is useful to find misspelled words. \n",
    "It uses a Levenshtein Distance algorithm to find permutations within an edit distance of 2 from the original word. It then compares all permutations (insertions, deletions, replacements, and transpositions) to known words in a word frequency list. Those words that are found more often in the frequency list are more likely the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde9779a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:20:43.350610Z",
     "iopub.status.busy": "2024-05-13T07:20:43.350239Z",
     "iopub.status.idle": "2024-05-13T07:21:15.965467Z",
     "shell.execute_reply": "2024-05-13T07:21:15.964549Z"
    },
    "papermill": {
     "duration": 32.625758,
     "end_time": "2024-05-13T07:21:15.967986",
     "exception": false,
     "start_time": "2024-05-13T07:20:43.342228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\r\n",
      "Installing collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.7.2\r\n"
     ]
    }
   ],
   "source": [
    "#Install pyspellcheck library to check for misspelled words\n",
    "!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdb738",
   "metadata": {
    "papermill": {
     "duration": 0.006688,
     "end_time": "2024-05-13T07:21:15.981947",
     "exception": false,
     "start_time": "2024-05-13T07:21:15.975259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Deobfuscation\n",
    "In this step we correct the text having spelling errors. For this a T5 deobfuscator is used as created in this [post.](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/457819) We are correcting only the texts which have more than 15 misspelled words.We are using hugging face's AutoTokenizer and AutoModel classes to use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d867979b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:21:15.997405Z",
     "iopub.status.busy": "2024-05-13T07:21:15.997090Z",
     "iopub.status.idle": "2024-05-13T07:21:16.004588Z",
     "shell.execute_reply": "2024-05-13T07:21:16.003707Z"
    },
    "papermill": {
     "duration": 0.017526,
     "end_time": "2024-05-13T07:21:16.006676",
     "exception": false,
     "start_time": "2024-05-13T07:21:15.989150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deobfuscate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deobfuscate.py\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from spellchecker import SpellChecker\n",
    "from nltk import word_tokenize\n",
    "import gc\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function to check the number of misspelled words\n",
    "def get_typos_count(input_text):\n",
    "    words = word_tokenize(input_text)\n",
    "    misspelled = spell.unknown(words)\n",
    "    return len(misspelled)\n",
    "\n",
    "# Function to correct the text with more than 15 misspelled words\n",
    "@torch.no_grad()\n",
    "def clean_essay(text):\n",
    "    doc = nlp(text)\n",
    "    #Tokenizing the words of text\n",
    "    inputs = tokenizer([s.text for s in doc.sents], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    #Generate outputs from the deobfuscator\n",
    "    outputs = deobfuscator.generate(inputs.input_ids.to(DEVICE), max_length=300)\n",
    "    #Detokenize the words so that it is readable\n",
    "    sents = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return \" \".join([s.strip() for s in sents])\n",
    "\n",
    "# Loading the deobfuscator model using huggingface Auto Classes\n",
    "MODEL_PATH = \"/kaggle/input/essay-gec/deobfuscator-v1\"\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "deobfuscator = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(DEVICE).eval()\n",
    "\n",
    "# Loading the english model from spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")\n",
    "test['n_typos'] = test['text'].apply(get_typos_count)\n",
    "clean_texts = []\n",
    "for i, r in tqdm(test.iterrows(), total=len(test)):\n",
    "    if r.n_typos < 15:\n",
    "        clean_texts.append(r.text)\n",
    "    else:\n",
    "        clean_texts.append(clean_essay(r.text))\n",
    "test[\"text\"] = clean_texts\n",
    "\n",
    "# Deleting the deobfuscator and texts to free up memory\n",
    "del deobfuscator, clean_texts\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Saving the clean essays\n",
    "test.to_csv('test_essays.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aaa26a",
   "metadata": {
    "papermill": {
     "duration": 0.006565,
     "end_time": "2024-05-13T07:21:16.020201",
     "exception": false,
     "start_time": "2024-05-13T07:21:16.013636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LLM predictions\n",
    "In this step we import the 12 deberta v3-large models trained to make predictions on the test data. Firstly we import the necessary classes from the source code to create several objects like configs,models,trainers and so on. Let's go through them step by step:\n",
    "\n",
    "**1.config**\n",
    "\n",
    "*  load_config: Loads the configurations of each model which are saved as a yaml file.\n",
    "*  dictionary_to_namespace: It is used to convert the configs from dictionary format to namespace format.\n",
    "  \n",
    "**2.data**\n",
    "* clean_text,clean_text2: These functions are used to remove symbols not present in original training set and also to normalize the remaining symbols.\n",
    "* Collator: The collator is a function that takes a list of samples, where each sample is the output of the __getitem__ method of your dataset class, and combines them into a batch. This function is specified as the collate_fn parameter when creating a DataLoader\n",
    "* CustomDataset: This class is built on top of the Dataset class of pytorch.\n",
    "\n",
    "**3.models**\n",
    "* CustomModel: Base class to build a pytorch neural network\n",
    "\n",
    "**4.training**\n",
    "* seed_everything: sets everything to agiven seed parameter\n",
    "* criterion: This specifies the loss function to be used\n",
    "* Trainer: This class is used for training and predicting the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169d7492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:21:16.035426Z",
     "iopub.status.busy": "2024-05-13T07:21:16.035170Z",
     "iopub.status.idle": "2024-05-13T07:21:16.041922Z",
     "shell.execute_reply": "2024-05-13T07:21:16.041097Z"
    },
    "papermill": {
     "duration": 0.016489,
     "end_time": "2024-05-13T07:21:16.043777",
     "exception": false,
     "start_time": "2024-05-13T07:21:16.027288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llm_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile llm_inference.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/llm-daig-src-code/src')##定位到源代码\n",
    "\n",
    "#Importing necessary functions from the source code used for training the model\n",
    "from config import load_config, dictionary_to_namespace\n",
    "from data import clean_text, clean_text2, make_text, CustomDataset, Collator\n",
    "from models import CustomModel\n",
    "from training import seed_everything, criterion, Trainer\n",
    "\n",
    "\n",
    "model_names = [\n",
    "    'exp222', 'exp200', 'exp184', 'exp179',\n",
    "    'exp477', 'exp478',\n",
    "    'exp489', 'exp492', 'exp510', 'exp512',\n",
    "    'exp500',\n",
    "    'exp511'\n",
    "]\n",
    "models_path = Path('/kaggle/input/llm-daig-final-models')\n",
    "\n",
    "for model_name in model_names:\n",
    "    df = pd.read_csv('test_essays.csv')\n",
    "    \n",
    "    #Loading configurations of each model\n",
    "    config = load_config(models_path / 'configs' / f'{model_name}.yaml')\n",
    "    config = dictionary_to_namespace(config)\n",
    "    seed_everything(config.seed)\n",
    "    \n",
    "    config.model.freeze_embeddings = False\n",
    "    config.dataset.max_length = 1512\n",
    "    \n",
    "    df['text_len'] = df['text'].str.len()\n",
    "    df = df.sort_values('text_len')\n",
    "    \n",
    "    #Remove symbols not present in original trainset\n",
    "    if model_name in ['exp222', 'exp200', 'exp184', 'exp179',]:\n",
    "        df['text'] = df['text'].apply(clean_text2)\n",
    "    else:\n",
    "        df['text'] = df['text'].apply(clean_text)\n",
    "    df = make_text(df, config)\n",
    "\n",
    "    #Create tokenizer of each model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models_path / 'tokenizer', use_fast=False)\n",
    "    config.tokenizer = tokenizer\n",
    "\n",
    "    dataset = CustomDataset(df, config, train=False)\n",
    "    collator = Collator(pad_to_multiple_of=0)\n",
    "\n",
    "    config.dataset.valid_batch_size = 4\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.dataset.valid_batch_size,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=collator,\n",
    "    )\n",
    "\n",
    "    backbone_config = AutoConfig.from_pretrained(models_path / 'backbone_configs' / f'{model_name}.json')\n",
    "    model_criterion = criterion.get_criterion(config)\n",
    "    model = CustomModel(\n",
    "        config, \n",
    "        init_from_config=True,\n",
    "        criterion=model_criterion,\n",
    "        backbone_config=backbone_config\n",
    "    )\n",
    "            \n",
    "    state = torch.load(\n",
    "        models_path / 'models' / f'{model_name}_weights.pth', \n",
    "        map_location=torch.device('cpu')\n",
    "    )\n",
    "    model.load_state_dict(state['model'])\n",
    "\n",
    "    # Making predictions after creating trainer\n",
    "    trainer = Trainer(model, config)\n",
    "    predictions = trainer.predict(dataloader)\n",
    "    \n",
    "    df['preds'] = predictions\n",
    "    exp_name = config.exp_name.split('_')[0]\n",
    "    df.to_csv(f'submission_{exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab088ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:21:16.058689Z",
     "iopub.status.busy": "2024-05-13T07:21:16.058428Z",
     "iopub.status.idle": "2024-05-13T07:26:35.647438Z",
     "shell.execute_reply": "2024-05-13T07:26:35.646298Z"
    },
    "papermill": {
     "duration": 319.599066,
     "end_time": "2024-05-13T07:26:35.649843",
     "exception": false,
     "start_time": "2024-05-13T07:21:16.050777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 5545.58it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.03s/it]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.61it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.27it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.88it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.40it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.97it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.44it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.16it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.79it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.76it/s]\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.93it/s]\r\n"
     ]
    }
   ],
   "source": [
    "!python deobfuscate.py\n",
    "!python llm_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7e394",
   "metadata": {
    "papermill": {
     "duration": 0.00976,
     "end_time": "2024-05-13T07:26:35.670099",
     "exception": false,
     "start_time": "2024-05-13T07:26:35.660339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pseudo labelling\n",
    "Pseudo labelling is the process through which we make predictions for some of the data in the test data and add these with the training set and train again. In this approach we are making predictions based on the initial 4 models trained on 11k dataset and considering only top 1000 samples which are closest to true samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d55ff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:26:35.691592Z",
     "iopub.status.busy": "2024-05-13T07:26:35.691261Z",
     "iopub.status.idle": "2024-05-13T07:26:36.174940Z",
     "shell.execute_reply": "2024-05-13T07:26:36.174192Z"
    },
    "papermill": {
     "duration": 0.497331,
     "end_time": "2024-05-13T07:26:36.177141",
     "exception": false,
     "start_time": "2024-05-13T07:26:35.679810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "exps = ['exp200', 'exp184', 'exp222', 'exp179']\n",
    "\n",
    "df = pd.read_csv(f'submission_{exps[0]}.csv', usecols=['id', 'text'])\n",
    "\n",
    "#Iterate through the initial 4 models and merge their predictions\n",
    "for exp in exps:\n",
    "    df_temp = pd.read_csv(f'submission_{exp}.csv')\n",
    "    df_temp = df_temp[['id', 'preds']]\n",
    "    df_temp.rename(columns={'preds': exp}, inplace=True)\n",
    "    df = pd.merge(df, df_temp, on='id', how='left')\n",
    "\n",
    "#Calculate mean of the predictions\n",
    "df['generated'] = df[exps].mean(axis=1)\n",
    "\n",
    "if df[(df['generated'] < 0.01) | (df['generated'] > 0.99)].shape[0] == 0:\n",
    "    df = df.head(1)\n",
    "else:\n",
    "    df = df[(df['generated'] < 0.01) | (df['generated'] > 0.99)]\n",
    "    \n",
    "    #Calculate distance from extremes for probabilities less than 0.01 and greater than 0.99\n",
    "    df.loc[(df['generated'] < 0.01), 'dist'] = df.loc[(df['generated'] < 0.01), 'generated']\n",
    "    df.loc[(df['generated'] > 0.99), 'dist'] = 1 - df.loc[(df['generated'] > 0.99), 'generated']\n",
    "    \n",
    "    df = df.sort_values('dist')\n",
    "    if df.shape[0] > 1000:\n",
    "        df = df.head(1000)\n",
    "\n",
    "#Pseudo label the top 1000 rows\n",
    "df.loc[(df['generated'] < 0.01), 'generated'] = 0\n",
    "df.loc[(df['generated'] > 0.99), 'generated'] = 1\n",
    "\n",
    "df = df[['text', 'generated']]\n",
    "df.rename(columns={'generated': 'label'}, inplace=True)\n",
    "\n",
    "df.to_csv('pseudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935f5b0",
   "metadata": {
    "papermill": {
     "duration": 0.009486,
     "end_time": "2024-05-13T07:26:36.196643",
     "exception": false,
     "start_time": "2024-05-13T07:26:36.187157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TF-IDF predictions\n",
    "This is the most popular approach used by many notebooks and is based on this [post](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/458522). \n",
    "\n",
    "Firstly we train a BPE tokenizer on the test data and then tokenize our training data. Then we vectorize the texts using a TF-IDF vectorizer with an ngram range of (3,5). The vectorized texts are then used to train an ensemble of classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4fbcb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:26:36.217565Z",
     "iopub.status.busy": "2024-05-13T07:26:36.217257Z",
     "iopub.status.idle": "2024-05-13T07:28:53.106986Z",
     "shell.execute_reply": "2024-05-13T07:28:53.106119Z"
    },
    "papermill": {
     "duration": 136.90321,
     "end_time": "2024-05-13T07:28:53.109512",
     "exception": false,
     "start_time": "2024-05-13T07:26:36.206302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fa28532a8348daad47702d9f68656d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c44675879f493faa1da26681942a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "sub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
    "org_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\n",
    "train = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\n",
    "pseudo = pd.read_csv('pseudo.csv')\n",
    "train = pd.concat([train, pseudo])\n",
    "\n",
    "train['text'] = train['text'].str.strip()\n",
    "test['text'] = test['text'].str.strip()\n",
    "\n",
    "train = train.drop_duplicates(subset=['text'])\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "LOWERCASE = False\n",
    "VOCAB_SIZE = 30522\n",
    "\n",
    "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
    "\n",
    "hq_pers = pd.read_csv('/kaggle/input/persaude-corpus-2/persuade_2.0_human_scores_demo_id_github.csv')\n",
    "hq_pers = hq_pers[hq_pers['holistic_essay_score'] > 4]\n",
    "hq_pers.rename(columns={'full_text': 'text'}, inplace=True)\n",
    "tokenizer_df = pd.concat([test, hq_pers])\n",
    "dataset = Dataset.from_pandas(tokenizer_df[['text']])\n",
    "\n",
    "def train_corp_iter(): \n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "        \n",
    "raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained('persuade_tokenizer')\n",
    "\n",
    "tokenized_texts_test = []\n",
    "for text in tqdm(test['text'].tolist()):\n",
    "    tokenized_texts_test.append(tokenizer.tokenize(text))\n",
    "\n",
    "tokenized_texts_train = []\n",
    "for text in tqdm(train['text'].tolist()):\n",
    "    tokenized_texts_train.append(tokenizer.tokenize(text))\n",
    "    \n",
    "def dummy(text):\n",
    "    return text\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n",
    "    tokenizer = dummy,\n",
    "    preprocessor = dummy,\n",
    "    token_pattern = None, strip_accents='unicode')\n",
    "\n",
    "\n",
    "vectorizer.fit(tokenized_texts_test)\n",
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n",
    "                            analyzer = 'word',\n",
    "                            tokenizer = dummy,\n",
    "                            preprocessor = dummy,\n",
    "                            token_pattern = None, strip_accents='unicode'\n",
    "                            )\n",
    "\n",
    "tf_train = vectorizer.fit_transform(tokenized_texts_train)\n",
    "tf_test = vectorizer.transform(tokenized_texts_test)\n",
    "\n",
    "del vectorizer\n",
    "gc.collect()\n",
    "\n",
    "y_train = train['label'].values\n",
    "\n",
    "if len(test.text.values) <= 5:\n",
    "    sub.to_csv('submission_960.csv', index=False)\n",
    "else:\n",
    "    clf = MultinomialNB(alpha=0.02)\n",
    "    sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \n",
    "    p6={'n_iter': 1750,'verbose': -1,'objective': 'binary','metric': 'auc','learning_rate': 0.05073909898961407, 'colsample_bytree': 0.726023996436955, 'colsample_bynode': 0.5803681307354022, 'lambda_l1': 8.562963348932286, 'lambda_l2': 4.893256185259296, 'min_data_in_leaf': 115, 'max_depth': 23, 'max_bin': 898}\n",
    "    lgb=LGBMClassifier(**p6)\n",
    "    cat=CatBoostClassifier(iterations=1250,\n",
    "                           verbose=0,\n",
    "                           l2_leaf_reg=6.6591278779517808,\n",
    "                           learning_rate=0.005689066836106983,\n",
    "                           allow_const_label=True,loss_function = 'CrossEntropy')\n",
    "    weights = [0.05,0.225,0.225,0.5]\n",
    " \n",
    "    ensemble = VotingClassifier(estimators=[('mnb',clf),\n",
    "                                            ('sgd', sgd_model),\n",
    "                                            ('lgb',lgb), \n",
    "                                            ('cat', cat)\n",
    "                                           ],\n",
    "                                weights=weights, voting='soft', n_jobs=-1)\n",
    "    ensemble.fit(tf_train, y_train)\n",
    "    gc.collect()\n",
    "    final_preds = ensemble.predict_proba(tf_test)[:,1]\n",
    "    sub['generated'] = final_preds\n",
    "    sub.to_csv('submission_960.csv', index=False)\n",
    "    sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70fbd15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:28:53.133144Z",
     "iopub.status.busy": "2024-05-13T07:28:53.132401Z",
     "iopub.status.idle": "2024-05-13T07:28:53.159731Z",
     "shell.execute_reply": "2024-05-13T07:28:53.159057Z"
    },
    "papermill": {
     "duration": 0.040934,
     "end_time": "2024-05-13T07:28:53.161667",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.120733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "exp_name = 'exp222'\n",
    "exps = ['exp200', 'exp184', 'exp222', 'exp179',]\n",
    "\n",
    "df = pd.read_csv(f'submission_{exp_name}.csv', usecols=['id', 'text'])\n",
    "\n",
    "for exp in exps:\n",
    "    df_temp = pd.read_csv(f'submission_{exp}.csv')\n",
    "    df_temp = df_temp[['id', 'preds']]\n",
    "    df_temp.rename(columns={'preds': exp}, inplace=True)\n",
    "    df = pd.merge(df, df_temp, on='id', how='left')\n",
    "\n",
    "#Average of 4 models trained on 11k dataset\n",
    "df['generated'] = df[exps].mean(axis=1)\n",
    "df[['id', 'generated',]].to_csv('submission_936.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8685f776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:28:53.184764Z",
     "iopub.status.busy": "2024-05-13T07:28:53.184465Z",
     "iopub.status.idle": "2024-05-13T07:28:53.227500Z",
     "shell.execute_reply": "2024-05-13T07:28:53.226734Z"
    },
    "papermill": {
     "duration": 0.056813,
     "end_time": "2024-05-13T07:28:53.229409",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.172596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model_names = [\n",
    "    '/kaggle/input/llm-daig-exp477',#finetune\n",
    "    '/kaggle/input/llm-daig-exp478',#finetune\n",
    "    \n",
    "    '/kaggle/input/llm-daig-exp489',\n",
    "    '/kaggle/input/llm-daig-exp492',#finetune\n",
    "    '/kaggle/input/llm-daig-exp510',#finetune\n",
    "    '/kaggle/input/llm-daig-exp512',#finetune\n",
    "    \n",
    "    '/kaggle/input/llm-daig-exp500', \n",
    "    \n",
    "    '/kaggle/input/llm-daig-exp511', #finetune\n",
    "]\n",
    "exps = [exp.split('-')[-1] for exp in model_names]\n",
    "\n",
    "df = pd.read_csv(f'submission_{exps[0]}.csv', usecols=['id', 'text'])\n",
    "\n",
    "for exp in exps:\n",
    "    df_temp = pd.read_csv(f'submission_{exp}.csv')\n",
    "    df_temp = df_temp[['id', 'preds']]\n",
    "    \n",
    "    df_temp.rename(columns={'preds': exp}, inplace=True)\n",
    "    df = pd.merge(df, df_temp, on='id', how='left')\n",
    "\n",
    "#Weighted average of 8 models\n",
    "df['generated'] = df[['exp512', 'exp510', 'exp492', 'exp489']].mean(axis=1) * 0.8 + df[['exp511', 'exp500', 'exp478', 'exp477']].mean(axis=1) * 0.2\n",
    "# df['generated'] = df['exp489'] * 0.8 + df[['exp500', 'exp477']].mean(axis=1) * 0.2\n",
    "# df['generated'] = df[['exp512', 'exp510', 'exp492']].mean(axis=1) * 0.8 + df[['exp511', 'exp500', 'exp478', 'exp477']].mean(axis=1) * 0.2\n",
    "df[['id', 'generated',]].to_csv('submission_959.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b487a8d",
   "metadata": {
    "papermill": {
     "duration": 0.010442,
     "end_time": "2024-05-13T07:28:53.250952",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.240510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fc4aba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:28:53.273075Z",
     "iopub.status.busy": "2024-05-13T07:28:53.272762Z",
     "iopub.status.idle": "2024-05-13T07:28:53.299412Z",
     "shell.execute_reply": "2024-05-13T07:28:53.298546Z"
    },
    "papermill": {
     "duration": 0.039949,
     "end_time": "2024-05-13T07:28:53.301424",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.261475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sub960 = pd.read_csv('submission_960.csv')\n",
    "sub936 = pd.read_csv('submission_936.csv')\n",
    "sub959 = pd.read_csv('submission_959.csv')\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
    "\n",
    "sub960.rename(columns={'generated': 'sub960'}, inplace=True)\n",
    "sub936.rename(columns={'generated': 'sub936'}, inplace=True)\n",
    "sub959.rename(columns={'generated': 'sub959'}, inplace=True)\n",
    "\n",
    "df = pd.merge(df, sub960[['id', 'sub960']], on='id', how='left')\n",
    "df = pd.merge(df, sub936[['id', 'sub936']], on='id', how='left')\n",
    "df = pd.merge(df, sub959[['id', 'sub959']], on='id', how='left')\n",
    "\n",
    "mask1 = (df['sub936'] > 0.1) & (df['sub936'] < 0.9)\n",
    "mask2 = (df['sub936'] < 0.1) | (df['sub936'] > 0.9)\n",
    "\n",
    "#Considering only tf-idf models for predictions between 0.1 and 0.9\n",
    "df.loc[mask1, 'generated'] = df.loc[mask1, 'sub936'] * 0.0 + df.loc[mask1, 'sub960'] * 1\n",
    "#Weighted average of tf-idf and 4 models if predictions below 0.1 and above 0.9\n",
    "# df.loc[mask2, 'generated'] = df.loc[mask2, 'sub936'] * 0.3 + df.loc[mask2, 'sub960'] * 0.7\n",
    "df.loc[mask2, 'generated'] = df.loc[mask2, 'sub936'] * 0.0 + df.loc[mask2, 'sub960'] * 1\n",
    "\n",
    "# Weighted average of all models\n",
    "# df['generated'] = df['generated'] * 0.85 + df['sub959'] * 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753947c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:28:53.324134Z",
     "iopub.status.busy": "2024-05-13T07:28:53.323795Z",
     "iopub.status.idle": "2024-05-13T07:28:53.338043Z",
     "shell.execute_reply": "2024-05-13T07:28:53.337153Z"
    },
    "papermill": {
     "duration": 0.02767,
     "end_time": "2024-05-13T07:28:53.339793",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.312123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>generated</th>\n",
       "      <th>sub960</th>\n",
       "      <th>sub936</th>\n",
       "      <th>sub959</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.467775</td>\n",
       "      <td>0.159715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.247500</td>\n",
       "      <td>0.148480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.408725</td>\n",
       "      <td>0.163375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  generated  sub960    sub936    sub959\n",
       "0  0000aaaa        0.1     0.1  0.467775  0.159715\n",
       "1  1111bbbb        0.9     0.9  0.247500  0.148480\n",
       "2  2222cccc        0.4     0.4  0.408725  0.163375"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba701aa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:28:53.362358Z",
     "iopub.status.busy": "2024-05-13T07:28:53.362079Z",
     "iopub.status.idle": "2024-05-13T07:28:53.368240Z",
     "shell.execute_reply": "2024-05-13T07:28:53.367347Z"
    },
    "papermill": {
     "duration": 0.019653,
     "end_time": "2024-05-13T07:28:53.370114",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.350461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[['id', 'generated']]\n",
    "# df.to_csv('submission_main.csv', index=False)\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debac8a",
   "metadata": {
    "papermill": {
     "duration": 0.01076,
     "end_time": "2024-05-13T07:28:53.391767",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.381007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Post processing\n",
    "\n",
    "For each prompt_id, if the number of samples there greater than 1000, we fitted umap on tfidfs (the same as in tfidf-catboost pipeline, but per-prompt), calculated distance to 7 closest human-written and 7 generated samples, and scaled predictions by the ratio human_distance / generated_distance with clipping to (0.9, 1.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cf38c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:28:53.414937Z",
     "iopub.status.busy": "2024-05-13T07:28:53.414637Z",
     "iopub.status.idle": "2024-05-13T07:28:53.421566Z",
     "shell.execute_reply": "2024-05-13T07:28:53.420727Z"
    },
    "papermill": {
     "duration": 0.020461,
     "end_time": "2024-05-13T07:28:53.423389",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.402928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd  \n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.cluster import KMeans\n",
    "# import umap\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "# preds = pd.read_csv('submission_main.csv')\n",
    "\n",
    "# test = pd.merge(test, preds, on='id', how='left')\n",
    "# #Using the tokenizer used for tf-idf model\n",
    "# tokenizer = AutoTokenizer.from_pretrained('persuade_tokenizer')\n",
    "\n",
    "# def dummy(text):\n",
    "#     return text\n",
    "    \n",
    "\n",
    "# dfs = []\n",
    "# for prompt_id in test.prompt_id.unique():\n",
    "#     #Creating dataframes corresponding to each prompt and also with predictions less than 0.2 and greater than 0.8\n",
    "#     sub = test[test['prompt_id'] == prompt_id].copy()\n",
    "#     nat = sub[sub['generated'] < 0.2]\n",
    "#     gen = sub[sub['generated'] > 0.8]\n",
    "    \n",
    "#     #Considering only the prompts with minimum 1000 samples\n",
    "#     if sub.shape[0] < 1000 or nat.shape[0] < 500 or gen.shape[0] < 500:\n",
    "#         sub['mult'] = 1\n",
    "#         dfs.append(sub)\n",
    "#         continue\n",
    "    \n",
    "#     #Get tokenized texts corresponding to each df\n",
    "#     tokenized_texts_test = []\n",
    "#     for text in tqdm(sub['text'].tolist()):\n",
    "#         tokenized_texts_test.append(tokenizer.tokenize(text))\n",
    "    \n",
    "#     nat_tokenized = []\n",
    "#     for text in tqdm(nat['text'].tolist()):\n",
    "#         nat_tokenized.append(tokenizer.tokenize(text))\n",
    "        \n",
    "#     gen_tokenized = []\n",
    "#     for text in tqdm(gen['text'].tolist()):\n",
    "#         gen_tokenized.append(tokenizer.tokenize(text))\n",
    "        \n",
    "\n",
    "#     vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n",
    "#         tokenizer = dummy,\n",
    "#         preprocessor = dummy,\n",
    "#         token_pattern = None, strip_accents='unicode')\n",
    "\n",
    "#     texts_tfidf = vectorizer.fit_transform(tokenized_texts_test)\n",
    "#     nat_tfidf = vectorizer.transform(nat_tokenized)\n",
    "#     gen_tfidf = vectorizer.transform(gen_tokenized)\n",
    "    \n",
    "#     #Get embeddings after creating umap vectorizer\n",
    "#     umap_vectorizer = umap.UMAP(random_state=2023, n_components=2).fit(texts_tfidf)\n",
    "#     embeddings = umap_vectorizer.transform(texts_tfidf)\n",
    "#     nat_embeddings = umap_vectorizer.transform(nat_tfidf)\n",
    "#     gen_embeddings = umap_vectorizer.transform(gen_tfidf)\n",
    "    \n",
    "#     multipliers = []\n",
    "#     for emb in embeddings:\n",
    "#         k = 7\n",
    "\n",
    "#         #Find distance of each embedding from human written and generated embeddings.\n",
    "#         nat_dist = np.sort(np.sum(np.square(emb - nat_embeddings), axis=1))\n",
    "#         gen_dist = np.sort(np.sum(np.square(emb - gen_embeddings), axis=1))\n",
    "\n",
    "#         if nat_dist[0] == 0:\n",
    "#             nat_dist = nat_dist[1:]\n",
    "#         else:\n",
    "#             gen_dist = gen_dist[1:]\n",
    "            \n",
    "#         #Calculate average of 7 closest distances\n",
    "#         nat_dist = nat_dist[:k].mean()\n",
    "#         gen_dist = gen_dist[:k].mean()\n",
    "\n",
    "#         #taking ratio of distances as multiplier\n",
    "#         mult = nat_dist / (gen_dist+1e-5)\n",
    "#         mult = min(mult, 1.25)\n",
    "#         mult = max(mult, 0.75)\n",
    "\n",
    "#         multipliers.append(mult)\n",
    "    \n",
    "#     sub['mult'] = multipliers\n",
    "#     dfs.append(sub)\n",
    "    \n",
    "# test = pd.concat(dfs)\n",
    "\n",
    "# #Multiply predictions between 0.1 and 0.9 with corresponding multipliers\n",
    "# mask = (test['generated'] > 0.1) & (test['generated'] < 0.9)\n",
    "# test.loc[mask, 'generated'] = test.loc[mask, 'generated'] * test.loc[mask, 'mult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a960a99d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:28:53.445644Z",
     "iopub.status.busy": "2024-05-13T07:28:53.445370Z",
     "iopub.status.idle": "2024-05-13T07:28:53.448967Z",
     "shell.execute_reply": "2024-05-13T07:28:53.448243Z"
    },
    "papermill": {
     "duration": 0.016773,
     "end_time": "2024-05-13T07:28:53.450757",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.433984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df6e52d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:28:53.472982Z",
     "iopub.status.busy": "2024-05-13T07:28:53.472675Z",
     "iopub.status.idle": "2024-05-13T07:28:53.476325Z",
     "shell.execute_reply": "2024-05-13T07:28:53.475486Z"
    },
    "papermill": {
     "duration": 0.016886,
     "end_time": "2024-05-13T07:28:53.478245",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.461359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test = test[['id', 'generated']]\n",
    "# test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26713048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-13T07:28:53.500962Z",
     "iopub.status.busy": "2024-05-13T07:28:53.500255Z",
     "iopub.status.idle": "2024-05-13T07:28:53.504139Z",
     "shell.execute_reply": "2024-05-13T07:28:53.503248Z"
    },
    "papermill": {
     "duration": 0.017154,
     "end_time": "2024-05-13T07:28:53.505984",
     "exception": false,
     "start_time": "2024-05-13T07:28:53.488830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3596984,
     "sourceId": 6258399,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3945154,
     "sourceId": 6865136,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3942644,
     "sourceId": 6890527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3960967,
     "sourceId": 6901341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937250,
     "sourceId": 7017419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4354596,
     "sourceId": 7480586,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4361897,
     "sourceId": 7497637,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 152459920,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 496.293682,
   "end_time": "2024-05-13T07:28:56.135802",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-13T07:20:39.842120",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "057f6dd4c6674b83989fd0c573184cac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "05e1a45657e741018f9e765f0cccc89b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4b47d1a92e0f4c7a9579ed3b070d2e5b",
       "placeholder": "​",
       "style": "IPY_MODEL_1084be773abf4a1ea26449a67162dfbc",
       "value": " 44865/44865 [01:11&lt;00:00, 634.75it/s]"
      }
     },
     "0cef0b9ff6764e44a2bf03ca6d16e387": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1084be773abf4a1ea26449a67162dfbc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "21f15c17f67f4cbeac9482c300d41385": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "22889f626e7c4c5eb124484afaa1af62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5e19d659b5854c43babdb65d56028276",
       "placeholder": "​",
       "style": "IPY_MODEL_7925f418e03143789621b306c35c9313",
       "value": "100%"
      }
     },
     "4b47d1a92e0f4c7a9579ed3b070d2e5b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e19d659b5854c43babdb65d56028276": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6868b33d3977484d87a986eaf02e576b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ff7c31faa0fe4868a39a368612f76698",
       "placeholder": "​",
       "style": "IPY_MODEL_21f15c17f67f4cbeac9482c300d41385",
       "value": "100%"
      }
     },
     "6c0af649e3f84a5e8cdba0921db31d08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0cef0b9ff6764e44a2bf03ca6d16e387",
       "max": 44865.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_befe9027649e4bb9b0a14dc46e9374ea",
       "value": 44865.0
      }
     },
     "6fb706d8b5c2420798d1924a303ea841": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74fa28532a8348daad47702d9f68656d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_22889f626e7c4c5eb124484afaa1af62",
        "IPY_MODEL_9107f59b436941f3a2f17984c780fd23",
        "IPY_MODEL_d65bec17fcb946eabd6f8aeb9d553da4"
       ],
       "layout": "IPY_MODEL_057f6dd4c6674b83989fd0c573184cac"
      }
     },
     "7925f418e03143789621b306c35c9313": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7e1d48b49d8d4fe38c498e7368cd8537": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8a313f0f5de248f38a8c9c5a08c00acf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9107f59b436941f3a2f17984c780fd23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a0c32528aab74ecf97be7d47024befce",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0bd50ad8fc7470b95243f35d9c29a31",
       "value": 3.0
      }
     },
     "a0c32528aab74ecf97be7d47024befce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a2c44675879f493faa1da26681942a74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6868b33d3977484d87a986eaf02e576b",
        "IPY_MODEL_6c0af649e3f84a5e8cdba0921db31d08",
        "IPY_MODEL_05e1a45657e741018f9e765f0cccc89b"
       ],
       "layout": "IPY_MODEL_8a313f0f5de248f38a8c9c5a08c00acf"
      }
     },
     "befe9027649e4bb9b0a14dc46e9374ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d0bd50ad8fc7470b95243f35d9c29a31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d65bec17fcb946eabd6f8aeb9d553da4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6fb706d8b5c2420798d1924a303ea841",
       "placeholder": "​",
       "style": "IPY_MODEL_7e1d48b49d8d4fe38c498e7368cd8537",
       "value": " 3/3 [00:00&lt;00:00, 212.25it/s]"
      }
     },
     "ff7c31faa0fe4868a39a368612f76698": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
